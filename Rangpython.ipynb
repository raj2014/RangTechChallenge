{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cust_status', 'object')\n",
      "('Trans24', 'object')\n",
      "('Trans25', 'object')\n",
      "('Trans26', 'object')\n",
      "('Trans27', 'object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reading input train and test data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Fill NA values\n",
    "train=train.fillna(value=-10)\n",
    "test=test.fillna(value=-10)\n",
    "\n",
    "# get response variable\n",
    "target=train['Active_Customer']\n",
    "train.drop(['Active_Customer','Cust_id'],axis=1,inplace=True)\n",
    "\n",
    "# saving test id\n",
    "testID=test['Cust_id']\n",
    "test.drop(['Cust_id'],axis=1,inplace=True)\n",
    "\n",
    "# Checking test and train descriptions\n",
    "#train.describe()\n",
    "#test.describe()\n",
    "\n",
    "# collecting the number of train instances\n",
    "num_train=train.shape[0]\n",
    "\n",
    "# concatenate train and test to a single object\n",
    "df=pd.concat([train,test])\n",
    "\n",
    "#df.describe()\n",
    "\n",
    "# Converting dtype of character objects to categorical values\n",
    "for column in df:\n",
    "    if str(df[column].dtype)=='object':\n",
    "        print(column,str(df[column].dtype))\n",
    "        df[column] = df[column].astype('category')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n"
     ]
    }
   ],
   "source": [
    "# Dummy encoding for the categorical variables\n",
    "# drop_first not set for 0.17 version of pandas\n",
    "df=pd.get_dummies(df,drop_first=True)\n",
    "df.describe()\n",
    "\n",
    "# Fill NA values\n",
    "df=df.fillna(value=-10)\n",
    "\n",
    "\n",
    "# Collecting the promotion columns\n",
    "promotion_index=[]\n",
    "id=0\n",
    "for column in df:\n",
    "    if column.find('Promotion')!=-1:\n",
    "        promotion_index.append(id)\n",
    "    id=id+1\n",
    "print id\n",
    "print promotion_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions,code for feature engineering\n",
    "\n",
    "def getCounts(x):\n",
    "    count=0\n",
    "    for i in x:\n",
    "        if i==0:\n",
    "            count=count+1\n",
    "    return count\n",
    "\n",
    "def naCounts(x):\n",
    "    count=0\n",
    "    for i in x:\n",
    "        if i==-10:\n",
    "            count=count+1\n",
    "    return count\n",
    "\n",
    "def getnum_promotions(x,promotion_index):\n",
    "    count=0\n",
    "    for i in promotion_index:\n",
    "        if x[i]!=-10 and x[i]>0:\n",
    "            count=count+1\n",
    "    return count\n",
    "\n",
    "def getsum_promotions(x,promotion_index):\n",
    "    sum=0\n",
    "    for i in promotion_index:\n",
    "        if x[i]!=-10 and x[i]>0:\n",
    "            sum=sum+x[i]\n",
    "    return sum\n",
    "\n",
    "#sum of zeros across the instances\n",
    "df['zeroCounts']=df.apply(getCounts,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#sum of NAs across the instances\n",
    "df['negativeCounts']=df.apply(naCounts,axis=1)\n",
    "\n",
    "\n",
    "#sum of promotions across the instances\n",
    "df['promotionCounts']=df.apply(lambda x : getnum_promotions(x,promotion_index),axis=1)\n",
    "\n",
    "\n",
    "#totalnumber of promotions not equal to zeros\n",
    "df['sum_promotionCounts'] =df.apply(lambda x : getsum_promotions(x,promotion_index),axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    36808.000000\n",
       "mean         0.747530\n",
       "std          1.356719\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.030000\n",
       "max         16.200000\n",
       "Name: sum_promotionCounts, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sum_promotionCounts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training a extra trees classifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=900,max_features= 80,criterion= 'entropy',min_samples_split= 2,\n",
    "                          max_depth= 60, min_samples_leaf= 2,random_state=1,\n",
    "                          verbose=1,n_jobs = 3)\n",
    "\n",
    "gbt = GradientBoostingClassifier(loss='deviance',learning_rate=0.02, subsample=0.9, \n",
    "                               random_state=1,max_depth=6, n_estimators=160,verbose=1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=3,max_features=90,max_depth=80,criterion='entropy',verbose=1)\n",
    "\n",
    "etr= ExtraTreesRegressor(n_estimators=700,max_features= 70,criterion= 'mse',min_samples_split= 2,\n",
    "                          max_depth= 60, min_samples_leaf= 2,random_state=2,\n",
    "                          verbose=1,n_jobs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid Search block for each classifier\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# param_grid = {\"max_features\": [90],\n",
    "#               \"criterion\":['entropy'],\n",
    "#               \"max_depth\":[80]\n",
    "#               }\n",
    "param_grid = {\"subsample\": [1],\n",
    "              \"max_depth\":[7,8]\n",
    "              }\n",
    "\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(gbt, param_grid=param_grid,scoring='accuracy',cv=5)\n",
    "#start = time()\n",
    "grid_search.fit(df.iloc[0:num_train,],target)\n",
    "for i in grid_search.grid_scores_:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check cross validation for single model\n",
    "\n",
    "# predicted = cross_validation.cross_val_predict(etr,df.iloc[0:num_train,],target, cv=5)\n",
    "# predicted=[i/predicted.max() for i in predicted]\n",
    "# for i in range(0,len(predicted)):\n",
    "#     if predicted[i]>0.5:\n",
    "#         predicted[i]=1\n",
    "#     else:\n",
    "#         predicted[i]=0\n",
    "# print metrics.accuracy_score(predicted,target)\n",
    "scores=cross_validation.cross_val_score(gbt,df.iloc[0:num_train,],target,cv=5,scoring='accuracy')\n",
    "print scores\n",
    "print(\"Accuracy: %f (+/- %f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 GradientBoostingClassifier(init=None, learning_rate=0.02, loss='deviance',\n",
      "              max_depth=6, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=160,\n",
      "              presort='auto', random_state=1, subsample=0.9, verbose=1,\n",
      "              warm_start=False)\n",
      "Fold 1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.3780           0.0074            1.41m\n",
      "         2           1.3702           0.0071            1.40m\n",
      "         3           1.3622           0.0055            1.39m\n",
      "         4           1.3552           0.0068            1.37m\n",
      "         5           1.3482           0.0060            1.36m\n",
      "         6           1.3408           0.0054            1.35m\n",
      "         7           1.3337           0.0046            1.34m\n",
      "         8           1.3275           0.0050            1.33m\n",
      "         9           1.3212           0.0048            1.32m\n",
      "        10           1.3145           0.0043            1.31m\n",
      "        20           1.2619           0.0028            1.25m\n",
      "        30           1.2215           0.0025            1.16m\n",
      "        40           1.1896           0.0016            1.06m\n",
      "        50           1.1622           0.0012           57.95s\n",
      "        60           1.1408           0.0009           53.01s\n",
      "        70           1.1196           0.0005           47.51s\n",
      "        80           1.1035           0.0001           41.75s\n",
      "        90           1.0903           0.0004           36.59s\n",
      "       100           1.0762           0.0002           31.21s\n",
      "Fold 2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.3778           0.0073            1.37m\n",
      "         2           1.3698           0.0076            1.36m\n",
      "         3           1.3619           0.0064            1.37m\n",
      "         4           1.3539           0.0062            1.36m\n",
      "         5           1.3468           0.0064            1.35m\n",
      "         6           1.3391           0.0055            1.35m\n",
      "         7           1.3322           0.0047            1.34m\n",
      "         8           1.3260           0.0051            1.33m\n",
      "         9           1.3193           0.0050            1.32m\n",
      "        10           1.3133           0.0054            1.31m\n",
      "        20           1.2603           0.0029            1.25m\n",
      "        30           1.2188           0.0028            1.14m\n",
      "        40           1.1849           0.0012            1.04m\n",
      "        50           1.1580           0.0010           57.15s\n",
      "        60           1.1352           0.0006           51.47s\n",
      "        70           1.1200           0.0006           45.99s\n",
      "        80           1.1007           0.0001           40.51s\n",
      "        90           1.0859           0.0001           35.20s\n",
      "       100           1.0772           0.0003           30.01s\n",
      "1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 900 out of 900 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=3)]: Done 900 out of 900 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=3)]: Done 900 out of 900 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=60, max_features=80, max_leaf_nodes=None,\n",
      "           min_samples_leaf=2, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=3,\n",
      "           oob_score=False, random_state=1, verbose=1, warm_start=False)\n",
      "Fold 1\n",
      "Fold"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 900 out of 900 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=3)]: Done 900 out of 900 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done 900 out of 900 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n",
      "2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=80, max_features=90, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=3,\n",
      "            oob_score=False, random_state=None, verbose=1,\n",
      "            warm_start=False)\n",
      "Fold 1\n",
      "Fold"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n",
      "3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=3)]: Done 700 out of 700 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 700 out of 700 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 700 out of 700 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=60,\n",
      "          max_features=70, max_leaf_nodes=None, min_samples_leaf=2,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=700, n_jobs=3, oob_score=False, random_state=2,\n",
      "          verbose=1, warm_start=False)\n",
      "Fold 1\n",
      "Fold"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:35: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=3)]: Done 700 out of 700 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 700 out of 700 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 700 out of 700 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    }
   ],
   "source": [
    "clfs=[gbt,et,rf,etr]\n",
    "\n",
    "# Working on Stacker\n",
    "print \"Creating train and test sets for blending.\"\n",
    "\n",
    "dataset_blend_train = np.zeros((train.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((test.shape[0], len(clfs)))\n",
    "\n",
    "\n",
    "# Selecting startified sampling\n",
    "nfolds=2\n",
    "skf = list(StratifiedKFold(target,nfolds))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "        print j, clf\n",
    "        dataset_blend_test_j = np.zeros((test.shape[0], len(skf)))\n",
    "        for i, (trainIndex, testIndex) in enumerate(skf):\n",
    "            print \"Fold\", i+1\n",
    "            X_train = df.iloc[trainIndex,]\n",
    "            y_train = target[trainIndex]\n",
    "            X_test = df.iloc[testIndex,]\n",
    "            y_test = target[testIndex]\n",
    "            clf.fit(X_train, y_train)\n",
    "            if j!=3:\n",
    "                y_submission = clf.predict_proba(X_test)[:,1]\n",
    "            else:\n",
    "                y_submission = clf.predict(X_test)\n",
    "                y_submission=[i/y_submission.max() for i in y_submission]\n",
    "            dataset_blend_train[testIndex, j] = y_submission\n",
    "            if j!=3:\n",
    "                dataset_blend_test_j[:, i] = clf.predict_proba(df.iloc[num_train:,])[:,1]\n",
    "            else:\n",
    "                y_submission = clf.predict(df.iloc[num_train:,])\n",
    "                y_submission = [i/y_submission.max() for i in y_submission]\n",
    "                dataset_blend_test_j[:, i]=y_submission\n",
    "        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69014358  0.68781529  0.68044237  0.67003106  0.68361801]\n",
      "Accuracy: 0.682410 (+/- 0.007037)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "scores=cross_validation.cross_val_score(clf,dataset_blend_train,target,cv=5,scoring='accuracy')\n",
    "print scores\n",
    "print(\"Accuracy: %f (+/- %f)\" % (scores.mean(), scores.std() * 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending with Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "print \"Blending with Logistic Regression\"\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, target)\n",
    "predicted = clf.predict_proba(dataset_blend_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted=[i/predicted.max() for i in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11042.000000\n",
       "mean         0.552610\n",
       "std          0.237213\n",
       "min          0.168935\n",
       "25%          0.337965\n",
       "50%          0.512333\n",
       "75%          0.761002\n",
       "max          1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predicted).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(predicted)):\n",
    "    if predicted[i]>0.5:\n",
    "        predicted[i]=1\n",
    "    else:\n",
    "        predicted[i]=0\n",
    "submission=pd.DataFrame({'Cust_id':testID,\n",
    "                        'Active_Customer':pd.Series(predicted)})\n",
    "submission.to_csv(\"submission_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
